{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f615309",
   "metadata": {},
   "source": [
    "## Preprocessing:\n",
    "    - Tokenization\n",
    "    - Lexical Analysis\n",
    "    - Stop word Elimination\n",
    "    - Stemming\n",
    "    - Lemmatization\n",
    "    - Vectorization\n",
    "\t\t- TF-IDF\n",
    "\t\t- WordEmbedding\n",
    "\t\t\t- BOW\n",
    "\t\t\t- CBOW\n",
    "\t\t\t- Word2Vec\n",
    "\t\t\t- GloVe\n",
    "    - Feature Selection\n",
    "        - Statistical Approaches\n",
    "            - Term Entropy Table (Select n terms with highest weight w(t): 1-entropy(t)) \n",
    "              entropy=Σd=1 to |D| -p(t)logp(t) base |D| with p(t) = tf t,D / Σd'∈ D tf t,d'\n",
    "            - tf-idf\n",
    "        - Semantical Approaches\n",
    "            - Named Entity Recognition approach using POS tagging \n",
    "            (EXtract Nouns with more semantic meaning i.e group nearby nouns that forms a concept)\n",
    "    - Feature Extraction\n",
    "        - Term Clustering\n",
    "        - Latent Semantic Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb3a602",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba71cd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'there', '.', 'hello', 'girl']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['won', '’', 't']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word tokenizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "sentence = \"hi there. hello girl\"\n",
    "tokens = word_tokenize(sentence)\n",
    "print(tokens)\n",
    "word_tokenize('won’t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc12609b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'can', \"'\", 't', 'allow', 'you', 'to', 'go', 'home', 'early']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#WordPunctTokenizer to handle punctuations\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokenizer.tokenize(\" I can't allow you to go home early\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "559e6d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi there.', 'hello girl.', \"can't\"]\n"
     ]
    }
   ],
   "source": [
    "#Sentence tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokens = sent_tokenize(\"hi there. hello girl. can't\")\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f113432f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"won't\", 'is', 'a', 'contraction']\n",
      "[\"can't\", 'is', 'a', 'contraction']\n"
     ]
    }
   ],
   "source": [
    "#Tokenization using regular expressions\n",
    "\n",
    "#customizable tokenisation, preferable for faster execution\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\") \n",
    "#matching alphanumeric tokens plus single quotes so that we don’t split contractions like “won’t”\n",
    "print(tokenizer.tokenize(\"won't is a contraction.\"))\n",
    "print(tokenizer.tokenize(\"can't is a contraction.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d124ffdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"won't is a contraction.\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenization using regular expressions with gaps=True\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('/s+' , gaps=True) #tokenize on whitespace, \n",
    "tokenizer.tokenize(\"won't is a contraction.\")\n",
    "# gaps = True means the pattern is going to identify the gaps to tokenize on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb674821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenization using regular expressions with gaps=False\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('/s+' , gaps=False)\n",
    "# if we will use gaps = False parameter then the pattern would be used to identify the tokens\n",
    "tokenizer.tokenize(\"won't is a contraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a159d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lorem ipsum dolor sit amet, consectetur adipiscing elit.', 'Praesent sit amet elementum mauris.', 'Curabitur finibus, velit eget lacinia tincidunt, tortor nisi ornare justo, in auctor enim nisi eu nulla.', 'Sed porta nibh vitae ante lobortis tempus.', 'Nullam auctor orci vitae volutpat venenatis.', 'Sed tristique lacus nisi, vitae faucibus erat mollis id.', 'Vivamus ac felis malesuada, interdum erat quis, mattis lorem.', 'Sed laoreet ut quam sed egestas.', 'Suspendisse potenti.', 'Nunc lacinia eros id quam ultricies, semper hendrerit lectus suscipit.', 'Maecenas eget orci purus.', 'Praesent diam quam, finibus ac viverra laoreet, volutpat vitae lectus.', 'Ut maximus magna leo, eu tincidunt nisl mattis non.', 'Vestibulum vitae nisl a ipsum eleifend malesuada.', 'Praesent porta, lectus a vulputate sodales, lorem ante venenatis nibh, a ultricies nisi erat pulvinar enim.', 'Quisque id eros sit amet risus hendrerit imperdiet.', 'Donec auctor mattis enim ut aliquam.', 'Maecenas et diam sit amet neque scelerisque varius a sit amet nisl.', 'Etiam vestibulum imperdiet ultrices.', 'Nulla odio turpis, dictum vel ornare id, imperdiet ut massa.', 'In hac habitasse platea dictumst.', 'Aliquam malesuada nunc nec orci auctor auctor at in turpis.', 'Vivamus maximus, neque sed fermentum tincidunt, ligula nunc fermentum ligula, mollis accumsan mi metus ac lorem.', 'Duis sit amet vestibulum arcu.', 'Mauris malesuada purus sed ligula porta malesuada.', 'Donec commodo posuere orci sit amet condimentum.', 'Nulla molestie nec magna tempus euismod.', 'Proin non suscipit velit.', 'Praesent at cursus justo.', 'Ut eu convallis felis, in aliquet velit.', 'Fusce feugiat, magna vitae finibus fermentum, justo nisi porttitor ante, sit amet tempus felis urna vitae leo.', 'Phasellus gravida, nulla non iaculis pretium, arcu felis egestas sem, at rutrum erat leo vitae mi.', 'Aliquam non molestie lectus, sed dignissim orci.', 'Aliquam auctor interdum velit, eget consectetur justo volutpat sed.', 'Aliquam malesuada vulputate enim non venenatis.', 'In in mauris felis.', 'Phasellus ex turpis, placerat a mi ut, consectetur lacinia dui.', 'Proin gravida sem velit, quis congue ligula fringilla eu.', 'Cras quis mauris sem.', 'Fusce sollicitudin non lorem ac pulvinar.', 'Nam consectetur erat nec nulla consequat, sit amet tincidunt elit congue.', 'Proin vulputate leo ut sollicitudin fermentum.', 'Pellentesque fringilla est non tincidunt dignissim.', 'Vivamus in posuere massa.', 'Donec tortor leo, vulputate eu nibh et, gravida imperdiet lectus.', 'Suspendisse dolor ligula, elementum vel aliquam a, congue non sem.', 'Vivamus sollicitudin ac elit a pretium.', 'Duis arcu mauris, pharetra et posuere lobortis, finibus at lectus.', 'Nulla facilisi.', 'Phasellus convallis lectus sem, vitae interdum massa sagittis ac.', 'Phasellus malesuada erat non tellus luctus rhoncus.', 'Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.', 'In id congue nisl.', 'Maecenas eget nunc ac massa finibus auctor.', 'Vestibulum arcu mauris, commodo quis pretium nec, interdum in enim.', 'Mauris imperdiet tellus imperdiet, interdum turpis vel, hendrerit ligula.', 'Mauris at gravida tellus.']\n",
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n"
     ]
    }
   ],
   "source": [
    "# Training own sentence tokenizer\n",
    "\n",
    "#for a text that is having a unique formatting.\n",
    "#To tokenize such text and get best results, we should train our own sentence tokenizer\n",
    "\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.corpus import webtext\n",
    "text = webtext.raw('E:\\Learning\\ML\\Learning Practice\\sampletext.txt') #getting the raw text\n",
    "sent_tokenizer = PunktSentenceTokenizer(text)\n",
    "sents_1 = sent_tokenizer.tokenize(text)\n",
    "print(sents_1)\n",
    "print(sents_1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5b875e",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "    It is the process of producing morphological variants of a root/base word.\n",
    "\tStemming is a technique used to extract the base form of the words by removing affixes from them.\n",
    "\tReduce words to their base/root form. eg: “chocolates”, “chocolatey”, “choco” to the root word, “chocolate”\n",
    "    “retrieval”, “retrieved”, “retrieves” reduce to the stem “retrieve”. \n",
    "    The input to the stemmer is tokenized words.\n",
    "    Why stemming? - \"normalize text and make it easier to process\" \n",
    "    Search engines use stemming for indexing the words.\n",
    "    That’s why rather than storing all forms of a word, a search engine can store only the stems. \n",
    "    In this way, stemming reduces the size of the index and increases retrieval accuracy\n",
    "    but could suffer with info loss, understemming, overstemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "004b164a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chocol'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PorterStemmer, LancasterStemmer\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "word_stemmer = PorterStemmer() #LancasterStemmer()\n",
    "word_stemmer.stem('chocolates')\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc91f4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bonjour'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SnowballStemmer\n",
    "#It supports 15 non-English languages. In order to use this steaming class, \n",
    "#we need to create an instance with the name of the language we are using and then call the stem() method. \n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "print(SnowballStemmer.languages)\n",
    "French_stemmer = SnowballStemmer('french')\n",
    "French_stemmer.stem('Bonjoura')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "118213df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Regular expression Stemmer\n",
    "#With the help of this stemming algorithm, we can construct our own stemmer.\n",
    "#It basically takes a single regular expression and removes any prefix or suffix that matches the expression\n",
    "import nltk\n",
    "from nltk.stem import RegexpStemmer\n",
    "Reg_stemmer = RegexpStemmer('ing')\n",
    "Reg_stemmer.stem('ingeat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d626b940",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\tLemmatization technique is like stemming. The output we will get after lemmatization is called ‘lemma’, which is a root word rather than root stem, the output of stemming.\n",
    "\t\t\t\n",
    "    After lemmatization, we will be getting a valid word. In simple words, stemming technique only looks at the form of the word whereas lemmatization technique looks at the meaning of the word. \n",
    "\t\n",
    "    It means after applying lemmatization, we will always get a valid word.\n",
    "\t\t\t\t\n",
    "\t\t\t- wordnet for lemmatization and word look up\n",
    "\t\t\t\t\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08b02cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'book'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('books')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9225480f",
   "metadata": {},
   "source": [
    "### Stop word elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0edf639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'writer']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stop word removal \n",
    "from nltk.corpus import stopwords\n",
    "stopwords.fileids() #gets the supported languages\n",
    "english_stops = set(stopwords.words('english'))\n",
    "words = ['I', 'am', 'a', 'writer']\n",
    "[word for word in words if word not in english_stops]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
