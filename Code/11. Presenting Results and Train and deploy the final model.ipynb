{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f05836b1",
   "metadata": {},
   "source": [
    "Ref: Machine Learning Mastery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ce3dc9",
   "metadata": {},
   "source": [
    "    Once you have found and tuned a viable model of your problem it is time to make use of that model. You may need to revisit your why and remind yourself what form you need a solution for the problem you are solving. The problem is not addressed until you do something with the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f929a0be",
   "metadata": {},
   "source": [
    "#### There are two main facets to making use of the results of your machine learning endeavor:\n",
    "\n",
    "    1. Report the results\n",
    "    2. Operationalize the system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11a64ec",
   "metadata": {},
   "source": [
    "### 1. Reporting the results\n",
    "    Once you have discovered a good model and a good enough result (or not, as the case may be), you will want to summarize what was learned and present it to stakeholders. This may be yourself, a client or the company for which you work.\n",
    "\n",
    "    Use a powerpoint template and address the sections listed below. You might like to write up a one-pager and use part section as a section header. Try to follow this process even on small experimental projects you do for yourself such as tutorials and competitions. It is easy to spend an inordinate number of hours on a project and you want to make sure to capture all the great things you learn along the way.\n",
    "\n",
    "    Below are the sections you can complete when reporting the results for a project.\n",
    "\n",
    "    > Context (Why): Define the environment in which the problem exists and set up the motivation for the research question.\n",
    "    > Problem (Question): Concisely describe the problem as a question that you went out and answered.\n",
    "    > Solution (Answer): Concisely describe the solution as an answer to the question you posed in the previous section. Be specific.\n",
    "    > Findings: Bulleted lists of discoveries you made along the way that interest the audience. They may be discoveries in the data, methods that did or did not work or the model performance benefits you achieved along your journey.\n",
    "    > Limitations: Consider where the model does not work or questions that the model does not answer. Do not shy away from these questions, defining where the model excels is more trusted if you can define where it does not excel.\n",
    "    > Conclusions (Why+Question+Answer): Revisit the why, research question and the answer you discovered in a tight little package that is easy to remember and repeat for yourself and others.\n",
    "    \n",
    "    The type of audience you are presenting to will define the amount of detail you go into. Having the discipline to complete your projects with a report on results, even on small side projects will accelerate your learning in field. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6196f8",
   "metadata": {},
   "source": [
    "### 2. Operationalize the model\n",
    "    \n",
    "    You have found a model that is good enough at addressing the problem you face that you would like to put it into production. This may be an operational installation on your workstation in the case of a fun side project, all the way up to integrating the model into an existing enterprise application. The scope is enormous. There are three key aspects to operationalizing a model that one could consider carefully before putting a system into production.\n",
    "    \n",
    "    Three areas that you should think carefully about are the \n",
    "    \n",
    "    > Algorithm implementation\n",
    "    > Automated testing of your model\n",
    "    > Tracking and monitoring of the models performance through time. \n",
    "    \n",
    "    These three issues will very likely influence the type of model you choose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941732b6",
   "metadata": {},
   "source": [
    "#### Algorithm Implementation\n",
    "    \n",
    "    It is likely that you were using a research library to discover the best performing method. The algorithm implementations in research libraries can be excellent, but they can also be written for the general case of the problem rather than the specific case with which you are working.\n",
    "\n",
    "    Think very hard about the dependencies and technical debt you may be creating by putting such an implementation directly into production. Consider locating a production-level library that supports the method you wish to use. You may have to repeat the process of algorithm tuning if you switch to a production level library at this point.\n",
    "\n",
    "    You may also consider implementing the algorithm yourself. This option may introduce risk depending on the complexity of the algorithm you have chosen and the implementation tricks it uses. Even with open source code, there may be a number of complex operations that may be very difficult to internalize and reproduce confidently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d1634e",
   "metadata": {},
   "source": [
    "#### Automated Model Tests\n",
    "    Write automated tests that verify that the model can be constructed and achieve a minimum level of performance repeatedly. Also write tests for any data preparation steps. You may wish to control the randomness used by the algorithm (random number seeds) for each unit test run so that tests are 100% reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923dc408",
   "metadata": {},
   "source": [
    "#### Tracking the model performance\n",
    "    Add infrastructure to monitor the performance of the model over time and raise alarms if accuracy drops below a minimum level. Tracking may occur in real-time or with samples of live data on a re-created model in a separate environment. A raised alarm may be an indication that that structure learned by the model in the data have changed (concept drift) and that the model may need to be updated or tuned.\n",
    "\n",
    "    Some models that perform retraining and tuning the params at scheduled times(once every week, once every day..).Track the performace for such auto update models. There are some model types that can perform online learning(real time live learning within every second) and update themselves. Think carefully in allowing models to update themselves in a production environment. In some circumstances, it can be wiser to manage the model update process and switch out models (their internal configuration) as they are verified to be more performant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714e459d",
   "metadata": {},
   "source": [
    "### Train the final model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69673684",
   "metadata": {},
   "source": [
    "Final model is the model that will be in the production or the model which we will use to make predictions for the new unknown data.\n",
    "\n",
    "The train-test split and k-fold cross validation are called resampling methods. Resampling methods are statistical procedures for sampling a dataset and estimating an unknown quantity.\n",
    "\n",
    "Cross validation is done to examine how the model is performing(w.r.to variance) and what are the best hyper parameters that the model could train on inorder to achieve the best results.\n",
    "\n",
    "While train-test split, we split the training data in training and test set.We are basically pretending that test set is the unseen data but actually its not.Purpose of test set is to assess the skill of the model.In other words, The model that is trained on training set is used for predictions on test set to find out the  actual generalised performance(skill) of the model on the unseen data.\n",
    "\n",
    "But,\n",
    "Once all the above steps are done, i.e once the model generalised performance is assessed, and the model configs are choosen,\n",
    "We will use the config i.e tuned hyperparameters, data preprocessing steps and the model to train on the entire existing training data(splitted training + test data).This is the final model that will be deployed to production for unseen real predictions.We can also create multiple final models and take the mean from an ensemble of predictions in order to reduce the variance.\n",
    "\n",
    "Note: We can discard the split datasets and the trained model.Also the cv models will be discarded at this point.They served their purpose and no longer neeeded.Only the final model is needed for production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e89f288",
   "metadata": {},
   "source": [
    "#### Tips on final model:\n",
    "    a) for classification you can use your final trained model with no risk\n",
    "    b) for regression, you have to rerun your model against all data (using the parameters tuned during training)\n",
    "    c) specifically for time series regression, you can’t use normal cross validation – it should respect the cronology of the data (from old to new always) and you have to rerun your model against all data (using the parameters tuned during training) as well, as the latest data are the crucial ones for the model to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b57851",
   "metadata": {},
   "source": [
    "### Deploy the model to production:\n",
    "    Below a five best practice steps one can follow to deploy the final predictive model into production.\n",
    "        1. Specify Performance Requirements.\n",
    "        2. Separate Prediction Algorithm From Model Coefficients.\n",
    "        3. Develop Regression Tests For Your Model.\n",
    "        4. Develop Back-Testing and Now-Testing Infrastructure.\n",
    "        5. Challenge Then Trial Model Updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf1e06f",
   "metadata": {},
   "source": [
    " 1. Specify Performance Requirements\n",
    "        You need to clearly spell out what constitutes good and bad performance. This maybe as accuracy or false positives or whatever metrics are important to the business. Spell out, and use the current model you have developed as the baseline numbers. These numbers may be increased over time as you improve the system. Performance requires are important. Without them, you will not be able to setup the tests you will need to determine if the system is behaving as expected. Do not proceed until you have agreed upon minimum, mean or a performance range expectation.Prerequisites in industrial machine learning should include logging, monitoring tools, switching on and off of features(feature flags)\n",
    "        \n",
    "        \n",
    "2. Separate Prediction Algorithm From Model Coefficients\n",
    "            You may have used a library to create your predictive model. For example, R, scikit-learn or Weka.You can choose to deploy your model using that library or re-implement the predictive aspect of the model in your software. You may even want to setup your model as a web service. Regardless, it is good practice to separate the algorithm that makes predictions from the model internals. That is the specific coefficients or structure within the model learned from your training data in a config file.\n",
    "           \n",
    "           2a. Select or Implement The Prediction Algorithm \n",
    "                Often the complexity a machine learning algorithms is in the model training, not in making predictions. For example, making predictions with a regression algorithm is quite straightforward and easy to implement in your language of choice. This would be an example of an obvious algorithm to re-implement rather than the library used in the training of the model.If you decide to use the library to make predictions, get familiar with the API and with the dependencies. The software used to make predictions is just like all the other software in your application.Treat it like software. Implement it well, write unit tests, make it robust.\n",
    "            \n",
    "            2b. Serialize Your Model Coefficients\n",
    "                Let’s call the numbers or structure learned by the model: coefficients.These data are not configuration for your application. Treat it like software configuration. Store it in an external file with the software project. Version it. Treat configuration like code because it can just as easily break your project. You very likely will need to update this configuration in the future as you improve your model.\n",
    "                \n",
    "                \n",
    "3. Develop Automated Tests For Your Model\n",
    "            You need automated tests to prove that your model works as you expect.In software land, we call these regression tests. They ensure the software has not regressed in its behavior in the future as we make changes to different parts of the system.\n",
    "            Write regression tests for your model.\n",
    "            Collect or contribute a small sample of data on which to make predictions.\n",
    "            Use the production algorithm code and configuration to make predictions.\n",
    "            Confirm the results are expected in the test.\n",
    "        These tests are your early warning alarm. If they fail, your model is broken and you can’t release the software or the features that use the model. Make the tests strictly enforce the minimum performance requirements of the model. It is strongly recommended for contriving test cases that you understand well, in addition to any raw datasets from the domain you want to include.Also it is strongly recommended gathering outlier and interesting cases from operations over time that produce unexpected results (or break the system). These should be understood and added to the regression test suite. Run the regression tests after each code change and before each release. Run them nightly.\n",
    "    \n",
    "4. Develop Back-Testing and Now-Testing Infrastructure\n",
    "        The model will change, as will the software and the data on which predictions are being made.You want to automate the evaluation of the production model with a specified configuration on a large corpus of data.This will allow you to efficiently back-test changes to the model on historical data and determine if you have truly made an improvement or not. This is not the small dataset that you may use for hyperparameter tuning, this is the full suite of data available, perhaps partitioned by month, year or some other important demarcation.Run the current operational model to baseline performance.Run new models, competing for a place to enter operations. Once set-up, run it nightly or weekly and have it spit out automatic reports.\n",
    "            Next, add a Now-Test.This is a test of the production model on the latest data. Perhaps it’s the data from today, this week or this month. The idea is to get an early warning that the production model may be faltering. This can be caused by content drift, where the relationships in the data exploited by your model are subtly changing with time. This Now-Test can also spit out reports and raise an alarm (by email) if performance drops below minimum performance requirements.\n",
    "        \n",
    "5. Challenge Then Trial Model Updates\n",
    "            You will need to update the model.Maybe you devise a whole new algorithm which requires new code and new config. Revisit all of the above points. A smaller and more manageable change would be to the model coefficients. For example, perhaps you set up a grid or random search of model hyperparameters that runs every night and spits out new candidate models.You should do this.Test the model and be highly critical. Give a new model every chance to slip up. Evaluate the performance of the new model using the Back-Test and Now-Test infrastructure in Point 4 above. Review the results carefully. Evaluate the change using the regression test, as a final automated check. Test the features of the software that make use of the model. Perhaps roll the change out to some locations or in a beta release for feedback, again for risk mitigation. Accept your new model once you are satisfied that it meets the minimum performance requirements and betters prior results. Like a ratchet, consider incrementally updating performance requirements as model performance improves.\n",
    "            \n",
    "            \n",
    "Industrial Machine Learning : https://www.youtube.com/watch?v=IgfRdDjLxe0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81d2d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
